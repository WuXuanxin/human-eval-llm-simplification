# human-eval-llm-simplification
We provide large-scale human annotations assessing LLM performance on sentence simplification:
| Evaluation Type        | Samples Annotated | Dimensions |
|------------------------|------------------|-------------|
| **Error Identification** | 4k+ complex-simple pairs | 7 error categories |
| **Likert-scale Rating** | 10k+ complex-simple pairs | Fluency, Meaning, Simplicity |

âš ï¸ IMPORTANT: In this repository, we excluded evaluations on the Newsela dataset due to licensing restrictions.
To obtain the complete evaluation data, please: 
1. Request access through Newsela's official website:  
   [https://newsela.com/data/](https://newsela.com/data/)
2. After obtaining permission, contact the author:  
  Xuanxin Wu [xuanxin.wu@ist.osaka-u.ac.jp](mailto:xuanxin.wu@ist.osaka-u.ac.jp)

Please refer to our paper for details of corpus creation: 
ðŸ”— [arXiv:2403.04963](https://arxiv.org/abs/2403.04963)
> ðŸ“œ Accepted at *ACM Transactions on Intelligent Systems and Technology* (to appear)  


Annotation files are available in the `/annotation` directory. Please refer to the README file for more information.
